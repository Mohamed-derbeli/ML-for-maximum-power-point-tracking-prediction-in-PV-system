{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6RkpsC8gdgwdvKImLYphe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-derbeli/ML-for-maximum-power-point-tracking-prediction-in-PV-system/blob/main/spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrWf6tzV99Cp"
      },
      "outputs": [],
      "source": [
        "# deep learning using spark (could be used for classification problem)\n",
        "\n",
        "# # # Number of Classes\n",
        "# # nb_classes= training_data.select(\"IMAX\").distinct().count()\n",
        "# # print(\"Number of classes = \", nb_classes)\n",
        "# # Number of Inputs or Input Dimentions\n",
        "# nb_inputs= len (training_data.select(\"Features_normalized\").first()[0])\n",
        "# print(\"Number of Inputs = \", nb_inputs)\n",
        "\n",
        "\n",
        "# !pip install elephas\n",
        "# # Elephas is an extension of Keras, which allows you to run distributed deep learning models at scale with Spark.\n",
        "# # Elephas for Deep Learning on Spark \n",
        "# from elephas.ml_model import ElephasEstimator\n",
        "# from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Set up deep learing Model/ Architecture\n",
        "\n",
        "# model2 = Sequential()\n",
        "# model2.add(Dense(10, activation=\"relu\", input_shape=(nb_inputs,))) \n",
        "# model2.add(Dense(10, activation=\"relu\"))\n",
        "# model2.add(Dense(10, activation=\"relu\"))\n",
        "# model2.add(Dense(1, activation=\"linear\")) \n",
        "# model2.compile(loss='mae', optimizer='rmsprop')\n",
        "\n",
        "# sgd = optimizers.SGD(lr=0.01)\n",
        "# opt_conf= optimizers.serialize(sgd)\n",
        "# # Initialize SparkML and estimator and get Settings\n",
        "# estimator= ElephasEstimator()\n",
        "# estimator.setFeaturesCol(\"Features_normalized\")\n",
        "# estimator.setLabelCol(\"IMAX\")\n",
        "# estimator.set_optimizer_config(opt_conf)\n",
        "# estimator.set_keras_model_config(model2.to_yaml)\n",
        "# estimator.set_batch_size(32)\n",
        "# estimator.set_epochs(10)\n",
        "# estimator.set_loss(\"mae\")\n",
        "# estimator. set_verbosity(2)\n",
        "\n",
        "# # Create Deep Learning Pipeline\n",
        "# dl_pipeline = Pipeline(stages=[estimator])\n",
        "\n",
        "# rdd=training_data.rdd\n",
        "\n",
        "# # fit model\n",
        "# # fit_dl_pipeline= dl_pipeline.fit(rdd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#upload data from local machine\n",
        "\n",
        "# uploaded = files.upload()  \n",
        "# data= io.BytesIO(uploaded['PV_all_data.xlsx'])\n",
        "# df = pd.read_excel(data)\n",
        "\n",
        "# upload Excel data from git\n",
        "# url = 'https://github.com/Mohamed-derbeli/ML-for-maximum-power-point-tracking-prediction-in-PV-system/blob/main/PV_data.xlsx?raw=true'\n",
        "\n",
        "# 2D\n",
        "# df.plot(x ='Vmax', y='Pmax', kind = 'scatter')\t\n",
        "# df.plot(x ='Vmax', y='Imax', kind = 'scatter')"
      ]
    }
  ]
}